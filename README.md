# ðŸ§  SmartDoc Assistant

SmartDoc Assistant is a GenAI-powered document comprehension tool that helps users:

- ðŸ“„ Summarize uploaded PDFs or TXT documents  
- â“ Ask contextual questions based on the uploaded document  
- ðŸ§  Take auto-generated quizzes to test understanding  

---

## âœ¨ Features

- Upload `.pdf` or `.txt` documents  
- Generate concise AI-based summaries  
- Ask questions and receive contextual answers with justifications  
- Generate logic-based quizzes  
- Get feedback on quiz answers  
- Persist previous uploads locally (via `localStorage`)  

---

## ðŸ§± Architecture & Reasoning Flow

Frontend (React) â—„â”€â”€â”€â–º Backend (FastAPI)

User Uploads File â”€â”€â”€â”€â”€â–º /upload
â””â”€â–º Extracts, chunks & indexes text
â””â”€â–º Embeds chunks for retrieval

User Asks Question â”€â”€â”€â”€â”€â–º /ask
â””â”€â–º Retrieves relevant chunks
â””â”€â–º Constructs prompt with context & history
â””â”€â–º Calls LLM (local or remote)

Generate Quiz â”€â”€â”€â”€â”€â–º /quiz/generate
â””â”€â–º Context-based question generation (LLM)

Evaluate Answers â”€â”€â”€â”€â”€â–º /quiz/evaluate
â””â”€â–º Feedback per answer using LLM

---


> ðŸ§  Uses embedding-based chunk retrieval for contextual Q&A and quiz generation. Context + session history helps create accurate prompts.

---

## âš™ï¸ Tech Stack

| Layer       | Tech                             |
|-------------|----------------------------------|
| Frontend    | React + Tailwind (optional)      |
| Backend     | FastAPI                          |
| Embedding   | SentenceTransformers / HF Models |
| LLM         | Local model / OpenAI compatible  |
| Storage     | Browser LocalStorage             |
| PDF Parsing | PyMuPDF / pdfminer               |

---

## ðŸš€ Setup Instructions

1ï¸âƒ£ Clone the Repository
git clone https://github.com/SurajitM0nd0l/SmartDoc-Assistant.git
cd SmartDoc-Assistant

2ï¸âƒ£ Backend Setup(/backend)
cd backend

Create virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

Install dependencies
pip install -r requirements.txt

Start FastAPI server
uvicorn main:app --reload
Runs at http://localhost:8000

3ï¸âƒ£ Frontend Setup (/frontend)
cd ../frontend

Install dependencies
npm install

Create .env file
echo "VITE_API_BASE_URL=http://localhost:8000" > .env

Start React app
npm run dev
Runs at http://localhost:5173

ðŸ§ª Example Workflow

Upload a research paper (PDF/TXT)
View the AI-generated summary
Ask contextual questions (e.g., "What is the core contribution?")
Generate a quiz â†’ Submit answers â†’ Receive intelligent feedback

ðŸ§° Local LLM with Ollama (Optional)

1. Install Ollama
Go to ollama.com/download and install for macOS, Windows, or Linux.
Verify installation:
ollama --version

3. Pull a Model
ollama pull mistral

5. Run the Model
ollama run mistral
